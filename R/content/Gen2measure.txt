\subsection{Generation 2} Administration of ability measures has varied considerably across the lifecourse of the NLSY-CYA survey (See Table 2.12 from \citealp{nlsy79datauser} for a summary). However, the vast majority of subjects have completed the following test batteries: \begin{itemize}
\item Peabody Individual Achievement Test (PIAT; \citealp{dunn1970peabody}):\begin{itemize}\item Math Subtest (84 items),
\item Reading Recognition Subtest (84 items),
\item Reading Comprehension Subtest (84 items),\end{itemize}
\item The Peabody Picture Vocabulary Test-Revised (PPVT-R; Form L; \citealp{dunn1981peabody}; 175 items), and
\item Wechsler Intelligence Scales for Children–Revised (WISC-R; \citealp{wechsler1974manual}) Digit Span Subscale (28 items).\end{itemize}
Although individual item level data was available for all of the aforementioned tests, conducting a unidimensional 2-PL is not a viable means of estimating general ability because of the nature of test construction and administration. The PIATs and PPVT-R were administered to subjects in an adaptive manner. The starting items on the PIAT Math and PVVT-R were determined by age, whereas the starting items for the remaining PIAT subtests were determined based on PIAT Math performance. Moreover, administration of a given test were terminated when a subject reached a ``ceiling.'' For example, testing was terminated for the PIAT Math if a subject incorrectly answered 5 of the most recent 7 questions (See \citealp{baker1993nlsy} for a thorough overview of NLSY-CYA test administration protocols). In essence, this administration procedure results in a tremendous amount of non-randomly missing data.

Although the administration created non-randomly missing data, the standard scores of the PVVT-R, PIATs, and WISC-R Digit Span themselves are valid and very reliable assessments of cognitive ability \citep{mott1995nlsy}. Accordingly, we elected to use the standard scores of all the Gen2 ability measures already mentioned. However, subjects were surveyed on a biannual basis. Thus we could not use cognitive tests at a given age. Instead, we aggregated scores across a 4 year window, and targeted ages 9 and 10. We targeted 9.5 because all cognitive tests were administered within the 8--11 age window, we wanted to maximize the number of subjects with viable ability scores, and we wanted to ensure temporal precedence with respect to AFI. In the case of missing subtests, we allowed age 11 scores to replace age 9 scores, and age 8 scores to replace age 10 scores. By employing a 4 year window, all subjects had an equal chance of replacing the primary test administration. Our replacement strategy ensured that the average age of testing matched the average of our targeted ages.

\subsubsection{Measurement} A unidimensional confirmatory factor analytic model was run in M\textit{plus} 7.31 \citep{mplus}, and used a robust maximum likelihood estimator (MLR). There were 8,254 useable observations in 3,742 clusters. A single factor solution fit the model decently (RMSEA $=$ .101, p(RMESA $< .05) = 0$; CFI $= .973$; TLI $= .946$, SRMR = .027). Table \vref{table_gen2measurement_9} contains a full summary of the model fit statistics, and Table \vref{table_g2loading_9} contains the factor loadings.

\subsubsection{Replicability \& Reliability} Given the recent concerns about replicatability in psychology \citep{OpenScienceCollaboration2015}, we repeated our aggregates of Gen2 intelligence, centered at ages 10.5 and 11.5, and replicated all of our analyses. These replications can be found in the Appendices \ref{appen10} and \ref{appen11}, respectively. Appendix \ref{appen10} begins on page \pageref{appen10} and appendix \ref{appen11} begins on page \pageref{appen11}. The test-retest reliabilities of Gen2 intelligence across our three aggregations is reported in the lower triangle of Table \vref{table_measurement_trt_g2int}. The diagonal indicates the number of respondents with intelligence aggregations for that year, and upper triangle reveal the number of respondents with viable scores for both respective ages. Stars indicate significant at the .01 level. The test-retest correlations are very high ( r > .90) across all pairings, suggesting that our method captures consistent (but not identical) measures of intelligence across ages. Additional analyses examining the reliability of intelligence difference scores are reported in a later section.