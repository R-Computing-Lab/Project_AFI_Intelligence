Although using the percentile of the AFQT standard score has been standard practice by the military and numerous researchers \citep{herrnstein1994bell,Der2009}, this practice is problematic on numerous fronts. In terms of consistency across NLSY studies, the AFQT percentile scores have been renormed twice --- in 1989 and 2006. The renorming of the AFQT has resulted in individual percentile scores fluctuating as much as 15\%. Within the BLS's database, those three different percentile scores are readily available, without consistent recommendation on which to use. The Department of Defense endorses the original normed scales, while the Center for Human Resource Research at Ohio State University recommends the 2006 renorming \citep{ing2012reanalysis}.


In terms of validity, researchers have found that the AFQT correlates more highly (.79, .99] with measures of literacy and reading comprehension \citep{Marks2010} than with the WAIS\citep{sticht1972project}. Moreover, the use of percentile scores is psychometrically problematic as it transforms an interval level measure into an ordinal level measure. Given these problems, inconsistent recommendations, and the recent release of the individual items of the AFQT, we have decided to estimate \textit{g} using item response theory. 

\subsubsection{Measurement}
Two unidimensional 2-PL Item Response Models were run in M\textit{plus} 7.31 \citep{muthen2014mplus}. One was estimated with a robust maximum likelihood estimator (MLR), and the other used robust weighted least squares(WLSMV). We chose to run both estimation methods because MLR is more computationally efficient, while WLSMV provides more accurate factor loading estimates \citep{li2014performance}. Because MLR is computationally efficient, we ran that estimation sequence first, and used those starting values for our WLSMV estimation. There were 11,172 useable observations in 7,669 clusters. Paragraph Comprehension item 15 was excluded from all analyses because of administration difficulties. PC15 was skipped by many subjects because it was the last question on the subtest and it was on the back of its own page (CITATION). A single factor solution fit the model well (RMESA $=$ .031 p(RMESA $< .05) = 1$; CFI $= .927$; TLI $= .925$). Table \vref{table_gen1measurement} contains a full summary of the model fit statistics by estimator. 

Figure \vref{G1wrightMap} contains a Wright Map. Probits have been rescaled by $\sqrt{1.6}$ into logits for ease of interpretation within the figure. Items are sorted by presentation order within subtests. Items become progressively harder within each subtest. There is better item coverage for low ability individuals for most items do not surpass thresholds greater than 0.5. The general ability of the participants is positively skewed. Inclusion of more difficult items would have likely reduced the skewedness. Given that this test was designed to screen out lower ability military applicants, the distributions of item difficulty served their intended purpose.


