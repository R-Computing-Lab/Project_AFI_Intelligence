\section{Measures}

\subsection{Generation 1}The Armed Services Vocational Aptitude Battery (ASVAB; Form 8A; \citealp{palmer1988armed}) was administered to Gen1 participants during the summer and fall of 1980 \citep{DoD1982}, and was used to establish national norms for the Department of Defense \citep{waters1987personnel}. The Armed Forces Qualification Test (AFQT) is derived from the ASVAB, and used as a measure of general trainability \citep{maier1986asvab}. It is a composite of four subscales: Arithmetic Reasoning (AR; 30 items), Math Knowledge (MK; 25 items), Paragraph Comprehension (PC; 15 items), and Word Knowledge (WK; 35 items). Arithmetic Reasoning targets the ability to solve word problems. Math Knowledge also tests quantitative ability, by assessing knowledge of high school level mathematics, with special emphasis on algebra, fractions, and geometry. The remaining subscales focus on verbal ability, and are sometimes referred to as the Verbal Composite (VE). Specifically, Word Knowledge tests the subjects' knowledge of the meaning of words within a given context, whereas Paragraph Comprehension targets a subject's ability to understand the meanings of paragraphs. Other administrations of the pencil and paper ASVAB reveal that all the AFQT subscales have high internal consistency ( $\alpha_{AR} = .91$; $\alpha_{WK} = .92$; $\alpha_{PC} = .81$; $\alpha_{MK} = .87$; \citealp{kass1982})


Methods of calculating the AFQT have varied throughout the ASVAB's administrative lifetime \citep{mayberry1992computing}. For pencil and paper administrations, standard scores were created for each of the subscale scores ($\bar{x}=50$, sd = 10), and then combined into a standard score. Then, the AFQT standard score is derived from the following formula:\begin{align}\text{AFQT} = \text{AR} + \text{MK} + 2\text{VE}, \\\text{where VE} = \text{PC} + \text{WK.}\end{align}

This score is then converted into a percentile, which determines an applicant's basic qualification for enlistment. All applicants must earn a score at or above the $10^{\text{th}}$percentile\citep{asvabtech4}. Each branch has its own minimum score, ranging from 31 to 36 \citep{army2013,UnitedStatesCoastGuard2004}, and each branch uses different linear combinations of these subtests to determine an applicant's eligibility for speciality positions. Additionally, multiple researchers have used the AFQT standard score as a proxy for general intelligence (\textit{g}) \citep{herrnstein1994bell,Der2009}. Indeed, the military has found that the AFQT correlated 0.8 with the Wechsler Adult Intelligence Scale (WAIS; \citealp{mcgrevy1974relationships}). Moreover, the AFQT consistently predicts outcomes traditionally associated with intelligence (See \citealp{Welsh1990}), including grades \citep{wilbourn1984,mathews1977analysis}.

\begin{comment}Although using the percentile of the AFQT standard score has been standard practice by the military and numerous researchers \citep{herrnstein1994bell,Der2009}, this practice is problematic on numerous fronts. In terms of consistency across NLSY studies, the AFQT percentile scores have been renormed twice --- in 1989 and 2006. The renorming of the AFQT has resulted in individual percentile scores fluctuating as much as 15\%. Within the BLS's database, those three different percentile scores are readily available, without consistent recommendation on which to use. The Department of Defense endorses the original normed scales, while the Center for Human Resource Research at Ohio State University recommends the 2006 renorming \citep{ing2012reanalysis}.


In terms of validity, researchers have found that the AFQT correlates more highly (.79, .99] with measures of literacy and reading comprehension \citep{Marks2010} than with the WAIS\citep{sticht1972project}. Moreover, the use of percentile scores is psychometrically problematic as it transforms an interval level measure into an ordinal level measure. Given these problems, inconsistent recommendations, and the recent release of the individual items of the AFQT, we have decided to estimate \textit{g} using item response theory. 

\subsubsection{Measurement}
Two unidimensional 2-PL Item Response Models were run in Mplus 7.3 \citep{muthen2014mplus}. One was estimated with a robust maximum likelihood estimator (MLR), and the other used robust weighted least squares(WLSMV). We chose to run both estimation methods because MLR is more computationally efficient, while WLSMV provides more accurate factor loading estimates \citep{li2014performance}. Because MLR is computationally efficient, we ran that estimation sequence first, and used those starting values for our WLSMV estimation. There were 11,172 useable observations in 7,669 clusters. Paragraph Comprehension item 15 was excluded from all analyses because of administration difficulties. PC15 was skipped by many subjects because it was the last question on the subtest and it was on the back of its own page (CITATION). A single factor solution fit the model well (RMESA $=$ .031 p(RMESA $< .05) = 1$; CFI $= .927$; TLI $= .925$). Table \vref{table_gen1measurement} contains a full summary of the model fit statistics by estimator. 

Figure \vref{G1wrightMap} contains a Wright Map. Probits have been rescaled by $\sqrt{1.6}$ into logits for ease of interpretation within the figure. Items are sorted by presentation order within subtests. Items become progressively harder within each subtest. There is better item coverage for low ability individuals for most items do not surpass thresholds greater than 0.5. The general ability of the participants is positively skewed. Inclusion of more difficult items would have likely reduced the skewedness. Given that this test was designed to screen out lower ability military applicants, the distributions of item difficulty served their intended purpose.\end{comment}


